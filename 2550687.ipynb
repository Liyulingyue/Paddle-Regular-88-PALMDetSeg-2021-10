{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 常规赛：PALM病理性近视病灶检测与分割\n",
    "## 具体介绍\n",
    "### 赛题介绍\n",
    "PALM病理性近视病灶检测与分割常规赛的重点是研究和发展与病理性近视诊断和患者眼底照片病变分割相关的算法。该常规赛的目标是评估和比较在一个常见的视网膜眼底图像数据集上检测病理性近视的自动算法。具体任务为：检测眼底图像是否出现视网膜萎缩病变和脱离病变，若有，需要实现病变区域的分割。[点击前往比赛界面](https://aistudio.baidu.com/aistudio/competition/detail/88/0/introduction)\n",
    "### 数据简介\n",
    "PALM病理性近视病灶检测与分割常规赛由中山大学中山眼科中心提供800张带萎缩和脱离病变分割标注的眼底彩照供选手训练模型，另提供400张带标注数据供平台进行模型测试。\n",
    "### 数据说明\n",
    "数据集中每个眼底彩照上都标注了与病理性近视相关的2种典型病变：斑片状视网膜萎缩(包括乳头周围萎缩)和视网膜脱离。像素级的病灶标注首先由中山大学中山眼科中心的7名眼科医生分别手动进行，最后由另一位高级专家将7位医生的标注结果融合为一个标注金标准，并存储为BMP图像。分割图像大小与对应的眼底图像大小相同，标签如下:</br>\n",
    "\t&ensp; 萎缩病变分割金标准：萎缩区域：0；背景：255；</br>\n",
    "\t&ensp; 脱离病变分割金标准：脱离区域：0；背景：255。</br>\n",
    "&ensp; 训练数据集</br>\n",
    "\t&ensp; &ensp; 文件名称：Train</br>\n",
    "\t&ensp; &ensp; Train里有两个文件夹，一个是fundus_images，一个是Lesion_Masks。</br>\n",
    "&ensp; &ensp; &ensp; fundus_images文件夹内包含800张眼底彩照，分辨率为1444×1444，或2124×2056。命名形如H0001.jpg、N0001.jpg、P0001.jpg和V0001.jpg。</br>\n",
    "&ensp; &ensp; &ensp; Lesion_Masks文件夹内包含两个文件夹：Atrophy和Detachment，其中，Atrophy文件夹包含fundus_images里眼底彩照的萎缩病变区域分割金标准，大小与对应的眼底彩照一致。命名前缀与对应眼底图像一致，后缀为bmp。同理，Detachment文件夹包含fundus_images里眼底彩照的脱离病变区域分割金标准，大小与对应的眼底彩照一致，命名前缀与对应眼底图像一致，后缀为bmp。请注意，若Lesion_Masks中无某张眼底图像的病灶分割结果，说明该图像中不包含对应的病灶区域。</br>\n",
    "&ensp; 测试数据集</br>\n",
    "\t&ensp; &ensp; 文件名称：PALM-Testing400-Images.zip</br>\n",
    "\t&ensp; &ensp; 压缩包里包含400张眼底彩照，命名形如T0001.jpg。</br>\n",
    "## 简要总结\n",
    "给定**一组**图片与**两种分割任务**在这组图片上的分割结果，要求训练对应的模型，对测试集分别给出这两种分割任务的**分割结果**。\n",
    "## 项目说明\n",
    "0.86278result.zip对应了我的最高得分结果。\n",
    "\n",
    "Detcsvfile/ResNet101_vd_ssld_10_6_3_6_0.83333_0.73032.csv对应了某次分类模型的运行结果，如没有特殊需求，请勿删除本csv文件，后面调用时会用到。\n",
    "\n",
    "[点击前往浏览测试记录](https://aistudio.baidu.com/aistudio/projectdetail/2435666)\t如果你对于如何从一个结果慢慢调整到一个比较好的结果的过程比较感兴趣的话，欢迎前往我的另外一个项目，这个项目和本项目几乎一致，但没有进行整理，里面纯粹是我的个人运行记录和总结，可能看起来比较乱(**直接点击链接在网页上会因为表格长度超出看不到总结的文字，可以复制粘贴或者在运行界面查看**)。但是也许你可以从中了解到一个并非大佬的人的逐渐修改项目调优的过程。**如果你认为这个测试记录是有价值的，便于你看懂的，欢迎你在之后项目中也公开自己的寻优过程促进大家的交流和思考~**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 方案设计思路\n",
    "## 大思路\n",
    "对于通常的分割任务，只需要设计分割模型进行处理。对于这一任务，给定的训练数据中，有大量的样本不需要被分割(即分割结果为全白)；对于预测样本，也有大量样本不需要被分割。因此，对于这个任务，有必要先构造分类模型，区分出样本是否有必要被分割，然后再构造分割模型进行分割。\n",
    "## 小细节\n",
    "对于Atrophy，大量样本都是需要被分割的，所以最终预测的时候，可以先分类，再分割；训练的时候，常规构造分类模型的训练，也可以将全部数据丢给分割模型进行训练。</br>\n",
    "对于Detachment，大量样本都是不需要分割的，所以最终预测的时候，**必须**先分类，再分割；训练的时候，分类的模型的构造要**注重数据的平衡**，**仅应使用应当被分割的样本**训练分割模型。\n",
    "## 版本说明（！非常重要！）\n",
    "大家打开就能看到的版本，应当对应了我的最高得分的版本\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/943d553dac87418e9bf1a70eeeb28b9d07c50e25e19a4f09804623cbbc4c4d1e)\n",
    "\n",
    "这个版本中使用了**非常丰富**的**非常规**操作，比如截取了他人的训练结果代替自己的部分运行结果，勾心斗角地计算预测集中需要Detachment的样本的数量，蓄意构造分类效果不是那么好的分类器，然后进行集成以提高最后得分。这些特殊的操作将在对应部分进行解释说明。\n",
    "\n",
    "总之，这个版本并不适合新手去跑通**分类+分割**的模型框架，因此我生成了**版本1**，虽然没有详细的描述，但是大致遵循了 **定义分类训练函数 - 定义分割训练函数 - 定义分类分割预测函数 - Atr训练分类分割&预测 - Det训练分类分割&预测 - 打包结果** ，应该不是特别难看懂，封包应该做的比较实在，看着函数名捋一遍应该对于新手来说不算太费劲。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 特别鸣谢与说明\n",
    "1. 感谢[yyyokay](https://aistudio.baidu.com/aistudio/personalcenter/thirdview/764763)的[项目](https://aistudio.baidu.com/aistudio/projectdetail/2276682)给不喜欢配置化训练的我提供了学习PaddleSeg调用式训练的机会。\n",
    "2. 感谢[Niki_173](https://aistudio.baidu.com/aistudio/personalcenter/thirdview/474269)在[飞桨常规赛：PALM病理性近视病灶检测与分割 2021 6月第3名方案](https://aistudio.baidu.com/aistudio/projectdetail/2099353)中保存下来了分割结果。因为我使用Unet进行Atr分割的结果不是特别好，所以我尝试参考之前的大佬们训练时使用的EMAnet，也许是我程序写的有问题，调用式使用EMAnet总是报错。因此，我直接从[飞桨常规赛：PALM病理性近视病灶检测与分割 2021 6月第3名方案](https://aistudio.baidu.com/aistudio/projectdetail/2099353)中下载分割结果，并将其中的Atr分割结果替代我自己训练的Unet网络在Atr部分的分割结果。该结果命名为Niki_1732021.8result.zip在本项目中使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 具体方案内容\n",
    "## 一些准备工作\n",
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 下载数据与解压数据\r\n",
    "import os\r\n",
    "import urllib\r\n",
    "\r\n",
    "url='https://bj.bcebos.com/v1/dataset-bj/%E5%8C%BB%E7%96%97%E6%AF%94%E8%B5%9B/%E5%B8%B8%E8%A7%84%E8%B5%9B%EF%BC%9APALM%E7%97%85%E7%90%86%E6%80%A7%E8%BF%91%E8%A7%86%E7%97%85%E7%81%B6%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%88%86%E5%89%B2.zip'\r\n",
    "\r\n",
    "if not os.path.exists('./work/Train_and_test.zip'):\r\n",
    "    print(\"Downloading start!\")\r\n",
    "    urllib.request.urlretrieve(url, \"./work/Train_and_test.zip\")  \r\n",
    "    print(\"Downloading end!\")\r\n",
    "else:\r\n",
    "    print(\"Already Downloading\")\r\n",
    "\r\n",
    "! unzip -oq ./work/Train_and_test.zip -d ./work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! unzip -oq /home/aistudio/Niki_1732021.8result.zip -d work/Niki_173"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 准备库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install \"numpy<=1.19.5\" -i https://mirror.baidu.com/pypi/simple\r\n",
    "!pip install \"paddlex==2.0.0\" -i https://mirror.baidu.com/pypi/simple\r\n",
    "!pip install paddleseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#导入常用的库\r\n",
    "import os\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "from random import shuffle\r\n",
    "import cv2\r\n",
    "import paddle\r\n",
    "from PIL import Image\r\n",
    "import shutil\r\n",
    "import re\r\n",
    "from paddle.vision.transforms import functional as F\r\n",
    "import os.path\r\n",
    "import paddleseg.transforms as T\r\n",
    "from paddleseg.datasets import OpticDiscSeg,Dataset\r\n",
    "import paddleseg.models\r\n",
    "from paddleseg.models import UNet\r\n",
    "from paddleseg.models import OCRNet\r\n",
    "from paddleseg.models.losses import CrossEntropyLoss,DiceLoss, MixedLoss\r\n",
    "from paddleseg.core import train\r\n",
    "from paddleseg.core import evaluate\r\n",
    "from paddleseg.core import predict\r\n",
    "from PIL import Image\r\n",
    "import paddlex as pdx\r\n",
    "import paddleseg as pds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 准备函数\n",
    "#### 分类函数\n",
    "PaddleX的分类训练是依赖于对应的txt文件的，因此本部分主要由以下几个自定义函数组成：\n",
    "\n",
    "1.\tlist2txtfile\t\t将list写入指定文件\n",
    "2.\tget_train_val_file\t划分list，构造训练集和验证集，不同的divide_type有不同的效果，可以促进det部分的训练样本平衡。\n",
    "3.\tgenerate_cls_file\t**重要函数**，用户直接调用这个函数生成对应的训练集txt，验证集txt\n",
    "4.\tgenerate_cls_model\t**重要函数**，用户直接调用这个函数生成对应的分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this code is created for cls-problems\r\n",
    "def list2txtfile(mylist,mydir):\r\n",
    "    with open(mydir,'w') as f:\r\n",
    "        for item in mylist:\r\n",
    "            f.write(item)\r\n",
    "\r\n",
    "def get_train_val_file(mylist,cut_percent=0.2,divide_type=0):\r\n",
    "    random.shuffle(mylist)\r\n",
    "    if divide_type==0:\r\n",
    "        cut_point=int(len(mylist)*cut_percent)\r\n",
    "        train=mylist[cut_point:]\r\n",
    "        val=mylist[:cut_point]\r\n",
    "    elif divide_type==1:\r\n",
    "        # 保持类别的等比划分\r\n",
    "        list0=[item for item in mylist if item[-2]=='0']\r\n",
    "        list1=[item for item in mylist if item[-2]=='1']\r\n",
    "        cut_point0=int(len(list0)*cut_percent)\r\n",
    "        cut_point1=int(len(list1)*cut_percent)\r\n",
    "        train=list0[cut_point0:]+list1[cut_point1:]\r\n",
    "        val=list0[:cut_point0]+list1[:cut_point1]\r\n",
    "        random.shuffle(train)\r\n",
    "        random.shuffle(val)\r\n",
    "    elif divide_type==2:\r\n",
    "        # 保持类别的等比划分,并且通过减少数据的方式抑制不平衡问题\r\n",
    "        list0=[item for item in mylist if item[-2]=='0']\r\n",
    "        list1=[item for item in mylist if item[-2]=='1']\r\n",
    "        list1=list1*int(64*2/len(list1))\r\n",
    "        list0=list0[:(64*6-len(list1))]\r\n",
    "        cut_point0=int(len(list0)*cut_percent)\r\n",
    "        cut_point1=int(len(list1)*cut_percent)\r\n",
    "        train=list0[cut_point0:]+list1[cut_point1:]\r\n",
    "        val=list0[:cut_point0]+list1[:cut_point1]\r\n",
    "        random.shuffle(train)\r\n",
    "        random.shuffle(val)\r\n",
    "    elif divide_type==3:\r\n",
    "        # 保持类别的等比划分,并且通过增加数据的方式抑制不平衡问题\r\n",
    "        list0=[item for item in mylist if item[-2]=='0']\r\n",
    "        list1=[item for item in mylist if item[-2]=='1']\r\n",
    "        list1=list1*int(len(list0)/len(list1))\r\n",
    "        random.shuffle(list0)\r\n",
    "        random.shuffle(list1)\r\n",
    "        cut_point0=int(len(list0)*cut_percent)\r\n",
    "        cut_point1=int(len(list1)*cut_percent)\r\n",
    "        train=list0[cut_point0:]+list1[cut_point1:]\r\n",
    "        val=list0[:cut_point0]+list1[:cut_point1]\r\n",
    "        random.shuffle(train)\r\n",
    "        random.shuffle(val)\r\n",
    "    return train,val\r\n",
    "\r\n",
    "def generate_cls_file(img_dir,lab_dir,out_dir,divide_type=0):\r\n",
    "    # img_dir='work/常规赛：PALM病理性近视病灶检测与分割/Train/fundus_image'\r\n",
    "    # lab_dir=work/常规赛：PALM病理性近视病灶检测与分割/Train/Lesion_Masks/Atrophy\r\n",
    "    # out_dir=work/\r\n",
    "    \r\n",
    "    os.makedirs(out_dir+'/cls/', exist_ok=True)\r\n",
    "\r\n",
    "    train_dir=out_dir+'/cls/train.txt'\r\n",
    "    eval_dir=out_dir+'/cls/eval.txt'\r\n",
    "    label_dir=out_dir+'/cls/label.txt'\r\n",
    "\r\n",
    "    img_names=[item.split('.')[0] for item in os.listdir(img_dir)]\r\n",
    "    lab_names=[item.split('.')[0] for item in os.listdir(lab_dir)]\r\n",
    "    cls_list=[]\r\n",
    "    for item in img_names:\r\n",
    "        if item in lab_names:\r\n",
    "            cls_list.append(img_dir+'/'+item+'.jpg 1\\n')\r\n",
    "        else:\r\n",
    "            cls_list.append(img_dir+'/'+item+'.jpg 0\\n')\r\n",
    "    list2txtfile(cls_list,out_dir+'/cls/ori_train_list.txt')\r\n",
    "    train,val=get_train_val_file(cls_list,divide_type=divide_type)\r\n",
    "    list2txtfile(train,out_dir+'/cls/train.txt')\r\n",
    "    list2txtfile(val,out_dir+'/cls/eval.txt')\r\n",
    "    list2txtfile(['0\\n1\\n'],out_dir+'/cls/label.txt')\r\n",
    "    \r\n",
    "    return train_dir,eval_dir,label_dir\r\n",
    "\r\n",
    "def generate_cls_model(train_dir,eval_dir,label_dir,out_dir,root_dir='/home/aistudio',num_epochs=100):\r\n",
    "    train_transforms = pdx.transforms.Compose([\r\n",
    "        #T.Resize([500,600], interp='LINEAR', keep_ratio=False),\r\n",
    "        pdx.transforms.RandomCrop(crop_size=224), \r\n",
    "        pdx.transforms.RandomHorizontalFlip(), \r\n",
    "        pdx.transforms.Normalize()])\r\n",
    "    eval_transforms = pdx.transforms.Compose([\r\n",
    "        #T.Resize([500,600], interp='LINEAR', keep_ratio=False),\r\n",
    "        pdx.transforms.ResizeByShort(short_size=256), \r\n",
    "        pdx.transforms.CenterCrop(crop_size=224), \r\n",
    "        pdx.transforms.Normalize()\r\n",
    "    ])\r\n",
    "\r\n",
    "    train_dataset = pdx.datasets.ImageNet(\r\n",
    "        data_dir=root_dir,\r\n",
    "        file_list=train_dir,\r\n",
    "        label_list=label_dir,\r\n",
    "        transforms=train_transforms,\r\n",
    "        shuffle=True)\r\n",
    "    eval_dataset = pdx.datasets.ImageNet(\r\n",
    "        data_dir=root_dir,\r\n",
    "        file_list=eval_dir,\r\n",
    "        label_list=label_dir,\r\n",
    "        transforms=eval_transforms)\r\n",
    "\r\n",
    "    num_classes = len(train_dataset.labels)\r\n",
    "    model = pdx.cls.ResNet101_vd_ssld(num_classes=num_classes)\r\n",
    "    model.train(num_epochs=num_epochs,\r\n",
    "                train_dataset=train_dataset,\r\n",
    "                train_batch_size=64,\r\n",
    "                eval_dataset=eval_dataset,\r\n",
    "                # lr_decay_epochs=[4, 6, 8],\r\n",
    "                save_interval_epochs=int(num_epochs/5),\r\n",
    "                learning_rate=0.025,\r\n",
    "                save_dir=out_dir+'/cls/model',\r\n",
    "                use_vdl=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 分割函数\n",
    "PaddleX的分类训练是依赖于对应的txt文件的，因此本部分主要由以下几个自定义函数组成：\n",
    "\n",
    "3.\tgenerate_det_file\t**重要函数**，用户直接调用这个函数生成对应的训练集txt，验证集txt\n",
    "4.\tgenerate_det_model\t**重要函数**，用户直接调用这个函数生成对应的分割模型\n",
    "\n",
    "* 分类函数里的get_train_val_file，list2txtfile在generate_det_file，generate_det_model里还会继续调用的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this code is create for det-problems, some functions from the previous block are re-used\r\n",
    "def generate_det_file(img_dir,lab_dir,out_dir):\r\n",
    "    # img_dir='work/常规赛：PALM病理性近视病灶检测与分割/Train/fundus_image'\r\n",
    "    # lab_dir=work/常规赛：PALM病理性近视病灶检测与分割/Train/Lesion_Masks/Atrophy\r\n",
    "    # out_dir=work/\r\n",
    "\r\n",
    "    os.makedirs(out_dir+'/det/png/', exist_ok=True)\r\n",
    "    \r\n",
    "    train_dir=out_dir+'/det/train.txt'\r\n",
    "    eval_dir=out_dir+'/det/eval.txt'\r\n",
    "    \r\n",
    "    img_names=[item.split('.')[0] for item in os.listdir(img_dir)]\r\n",
    "    lab_names=[item.split('.')[0] for item in os.listdir(lab_dir)]\r\n",
    "    det_list=[]\r\n",
    "    for item in lab_names:\r\n",
    "        im=Image.open(lab_dir+'/'+item+'.bmp')\r\n",
    "        im=(np.array(im)/255).astype('uint8')\r\n",
    "        im = Image.fromarray(im)\r\n",
    "        im.save(out_dir+'/det/png/'+item+'.png')\r\n",
    "        det_list.append(img_dir+'/'+item+'.jpg '+out_dir+'/det/png/'+item+'.png'+'\\n')\r\n",
    "    list2txtfile(det_list,out_dir+'/det/ori_train_list.txt')\r\n",
    "    train,val=get_train_val_file(det_list)\r\n",
    "    list2txtfile(train,out_dir+'/det/train.txt')\r\n",
    "    list2txtfile(det_list,out_dir+'/det/eval.txt')\r\n",
    "    #list2txtfile(val,out_dir+'/det/eval.txt')\r\n",
    "\r\n",
    "    return train_dir,eval_dir\r\n",
    "\r\n",
    "def generate_det_model(train_dir,eval_dir,out_dir,root_dir='/home/aistudio',iters=2000):\r\n",
    "    train_transforms = [\r\n",
    "        pds.transforms.ResizeStepScaling(min_scale_factor=0.5,max_scale_factor=2.0,scale_step_size=0.25),\r\n",
    "        pds.transforms.Resize(target_size=(800,800)),\r\n",
    "        pds.transforms.Normalize()  # 图像标准化\r\n",
    "    ]\r\n",
    "    val_transforms = [\r\n",
    "        pds.transforms.Resize(target_size=(800,800)),\r\n",
    "        pds.transforms.Normalize()\r\n",
    "    ]\r\n",
    "\r\n",
    "    # 构建训练集\r\n",
    "    train_dataset = pds.datasets.Dataset(\r\n",
    "        dataset_root=root_dir,\r\n",
    "        train_path=train_dir,\r\n",
    "        transforms=train_transforms,\r\n",
    "        num_classes=2,\r\n",
    "        mode='train'\r\n",
    "    )\r\n",
    "    #验证集\r\n",
    "    val_dataset = pds.datasets.Dataset(\r\n",
    "        dataset_root=root_dir,\r\n",
    "        val_path=eval_dir,\r\n",
    "        transforms=val_transforms,\r\n",
    "        num_classes=2,\r\n",
    "        mode='val'\r\n",
    "    )\r\n",
    "\r\n",
    "    model = pds.models.UNet(num_classes=2)\r\n",
    "\r\n",
    "    base_lr =0.001\r\n",
    "\r\n",
    "    #自动调整学习率\r\n",
    "    lr =paddle.optimizer.lr.CosineAnnealingDecay(base_lr, T_max=(iters // 3), last_epoch=0.5)\r\n",
    "    u_optimizer = paddle.optimizer.Adam(lr, parameters=model.parameters())\r\n",
    "\r\n",
    "    mixtureLosses = [pds.models.losses.CrossEntropyLoss(),pds.models.losses.DiceLoss() ]\r\n",
    "    mixtureCoef = [1.0,0.0]\r\n",
    "    losses = {}\r\n",
    "    losses['types'] = [pds.models.losses.MixedLoss(mixtureLosses, mixtureCoef)]\r\n",
    "    losses['coef'] = [1]\r\n",
    "\r\n",
    "    pds.core.train(\r\n",
    "        model = model,\r\n",
    "        train_dataset=train_dataset,\r\n",
    "        val_dataset=val_dataset,\r\n",
    "        optimizer=u_optimizer,\r\n",
    "        save_dir=out_dir+'/det/model',\r\n",
    "        iters=iters,  \r\n",
    "        batch_size=8, \r\n",
    "        save_interval=int(iters/5),\r\n",
    "        log_iters=40,\r\n",
    "        num_workers=2,\r\n",
    "        losses=losses,\r\n",
    "        use_vdl=False\r\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 分类-分割预测\n",
    "\n",
    "本部分主要由以下函数组成：\n",
    "\n",
    "1.\tmyremovedirs 清空对应目录，保持~~空气清新~~生成的结果不会和之前的结果重合\n",
    "2.\tgenerate_white_pic 对于不需要分割的图片，直接按照图片尺寸生成全白像素图片作为分割结果\n",
    "3.\tpredict_img\t根据对应的模型进行分类和分割，将分类结果中不需要分割的生成全白图片，需要分割的生成对应的分割结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this code is created for cls and det predictions\r\n",
    "def myremovedirs(mydir):\r\n",
    "    if not os.path.exists(mydir): return\r\n",
    "    try: # 删除文件\r\n",
    "        os.remove(mydir)\r\n",
    "    except: # 文件删除失败可能为目录\r\n",
    "        try:\r\n",
    "            os.removedirs(mydir)\r\n",
    "        except: # 可能不是空目录\r\n",
    "            for item in os.listdir(mydir):\r\n",
    "                myremovedirs(mydir+'/'+item)\r\n",
    "            os.removedirs(mydir)\r\n",
    "\r\n",
    "def generate_white_pic(white_list,result_dir):\r\n",
    "    for item in white_list:\r\n",
    "        im=Image.open(item)\r\n",
    "        row=np.array(im).shape[0]\r\n",
    "        col=np.array(im).shape[1]\r\n",
    "        im=(np.ones([row,col],dtype='int64')*255).astype('uint8')\r\n",
    "        im = Image.fromarray(im)\r\n",
    "        im.save(result_dir+'/'+item.split('/')[-1].split('.')[0]+'.png')\r\n",
    "\r\n",
    "def predict_img(test_dir,out_dir,result_dir,cls_model_dir=\"\"):\r\n",
    "    if cls_model_dir==\"\":\r\n",
    "        cls_model_dir=out_dir+'/cls/model/best_model'\r\n",
    "    \r\n",
    "    # 清空目录\r\n",
    "    myremovedirs(out_dir+'/pseudo_color_prediction')\r\n",
    "\r\n",
    "    os.makedirs(result_dir, exist_ok=True)\r\n",
    "    \r\n",
    "    test_list=[]\r\n",
    "    white_list=[]\r\n",
    "\r\n",
    "    cls_model = pdx.load_model(cls_model_dir)\r\n",
    "    for item in os.listdir(test_dir):\r\n",
    "        fig_name=item.split('.')[0]\r\n",
    "        tmp=cls_model.predict(test_dir+'/'+item)[0]['category']\r\n",
    "        if tmp=='1':\r\n",
    "            test_list.append(test_dir+'/'+fig_name+'.jpg')\r\n",
    "        else:\r\n",
    "            white_list.append(test_dir+'/'+fig_name+'.jpg')\r\n",
    "    \r\n",
    "    print('empty img are '+str(len(white_list))+\", unempty img are \"+str(len(test_list)))\r\n",
    "\r\n",
    "    generate_white_pic(white_list,result_dir)\r\n",
    "\r\n",
    "    transforms = pds.transforms.Compose([\r\n",
    "        pds.transforms.Resize(target_size=(800, 800)),\r\n",
    "        pds.transforms.Normalize()\r\n",
    "        ])\r\n",
    "    \r\n",
    "    model = pds.models.UNet(num_classes=2)\r\n",
    "\r\n",
    "    predict(\r\n",
    "        model,\r\n",
    "        model_path = out_dir+'/det/model/best_model/model.pdparams',\r\n",
    "        transforms=transforms,\r\n",
    "        image_list=test_list,\r\n",
    "        save_dir=out_dir,)\r\n",
    "\r\n",
    "    for img in os.listdir(out_dir+'/pseudo_color_prediction'):\r\n",
    "        img_dir=out_dir+'/pseudo_color_prediction/'+img\r\n",
    "        im=Image.open(img_dir)\r\n",
    "        im=Image.fromarray(np.array(im)*255)\r\n",
    "        im.save(result_dir+'/'+img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 基于Niki_1732021.8result.zip的Atrophy分类-分割预测\n",
    "\n",
    "predict_img2\t根据分类模型，将不需要分割的生成对应的全白图片，根据需要分割的图片编号，提取Niki_1732021.8result.zip中对应的分割结果。这一函数主要用于Atr的分类-分割预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_img2(test_dir,out_dir,result_dir,cls_model_dir=\"\",predicted_dir=\"work/Niki_173/Lesion_Segmentation/Atrophy\"):\r\n",
    "    if cls_model_dir==\"\":\r\n",
    "        cls_model_dir=out_dir+'/cls/model/best_model'\r\n",
    "    \r\n",
    "    # 清空目录\r\n",
    "    myremovedirs(out_dir+'/pseudo_color_prediction')\r\n",
    "\r\n",
    "    os.makedirs(result_dir, exist_ok=True)\r\n",
    "    \r\n",
    "    test_list=[]\r\n",
    "    white_list=[]\r\n",
    "\r\n",
    "    cls_model = pdx.load_model(cls_model_dir)\r\n",
    "    for item in os.listdir(test_dir):\r\n",
    "        fig_name=item.split('.')[0]\r\n",
    "        tmp=cls_model.predict(test_dir+'/'+item)[0]['category']\r\n",
    "        if tmp=='1':\r\n",
    "            test_list.append(test_dir+'/'+fig_name+'.jpg')\r\n",
    "        else:\r\n",
    "            white_list.append(test_dir+'/'+fig_name+'.jpg')\r\n",
    "    \r\n",
    "    print('empty img are '+str(len(white_list))+\", unempty img are \"+str(len(test_list)))\r\n",
    "\r\n",
    "    generate_white_pic(white_list,result_dir)\r\n",
    "    \r\n",
    "    test_names=[img.split('/')[-1].split('.')[0] for img in test_list]\r\n",
    "    for img in os.listdir(predicted_dir):\r\n",
    "        if img.split('.')[0] in test_names:\r\n",
    "            img_dir=predicted_dir+'/'+img\r\n",
    "            im=Image.open(img_dir)\r\n",
    "            im.save(result_dir+'/'+img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Atrophy分类&分割\n",
    "### Atrophy分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_dir='work/常规赛：PALM病理性近视病灶检测与分割/Train/fundus_image'\r\n",
    "lab_dir='work/常规赛：PALM病理性近视病灶检测与分割/Train/Lesion_Masks/Atrophy'\r\n",
    "out_dir='work/Atrophy'\r\n",
    "root_dir='/home/aistudio'\r\n",
    "\r\n",
    "train_dir,eval_dir,label_dir=generate_cls_file(img_dir,lab_dir,out_dir)\r\n",
    "generate_cls_model(train_dir,eval_dir,label_dir,out_dir,root_dir=root_dir,num_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Atrophy分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_dir='work/常规赛：PALM病理性近视病灶检测与分割/Train/fundus_image'\r\n",
    "lab_dir='work/常规赛：PALM病理性近视病灶检测与分割/Train/Lesion_Masks/Atrophy'\r\n",
    "out_dir='work/Atrophy'\r\n",
    "root_dir='/home/aistudio'\r\n",
    "\r\n",
    "train_dir,eval_dir=generate_det_file(img_dir,lab_dir,out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 由于分割的数据集准备太花时间，所以直接通过参数配置调用即可\r\n",
    "train_dir='work/Atrophy/det/train.txt'\r\n",
    "eval_dir='work/Atrophy/det/eval.txt'\r\n",
    "out_dir='work/Atrophy'\r\n",
    "root_dir='/home/aistudio'\r\n",
    "\r\n",
    "generate_det_model(train_dir,eval_dir,out_dir,root_dir=root_dir,iters=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Atrophy预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_dir='work/Atrophy'\r\n",
    "test_dir='work/常规赛：PALM病理性近视病灶检测与分割/PALM-Testing400-Images'\r\n",
    "result_dir='Lesion_Segmentation/Atrophy'\r\n",
    "# predict_img(test_dir,out_dir,result_dir)\r\n",
    "predict_img2(test_dir,out_dir,result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Detachment分割&分割\n",
    "### Detachment分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_dir='work/常规赛：PALM病理性近视病灶检测与分割/Train/fundus_image'\r\n",
    "lab_dir='work/常规赛：PALM病理性近视病灶检测与分割/Train/Lesion_Masks/Detachment'\r\n",
    "out_dir='work/Detachment'\r\n",
    "root_dir='/home/aistudio'\r\n",
    "\r\n",
    "train_dir,eval_dir,label_dir=generate_cls_file(img_dir,lab_dir,out_dir,divide_type=3)\r\n",
    "generate_cls_model(train_dir,eval_dir,label_dir,out_dir,root_dir=root_dir,num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Detachment分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_dir='work/常规赛：PALM病理性近视病灶检测与分割/Train/fundus_image'\r\n",
    "lab_dir='work/常规赛：PALM病理性近视病灶检测与分割/Train/Lesion_Masks/Detachment'\r\n",
    "out_dir='work/Detachment'\r\n",
    "root_dir='/home/aistudio'\r\n",
    "\r\n",
    "train_dir,eval_dir=generate_det_file(img_dir,lab_dir,out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "generate_det_model(train_dir,eval_dir,out_dir,root_dir=root_dir,iters=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Detachment预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_dir='work/Detachment'\r\n",
    "test_dir='work/常规赛：PALM病理性近视病灶检测与分割/PALM-Testing400-Images'\r\n",
    "result_dir='Lesion_Segmentation/Detachment'\r\n",
    "# cls_model_dir=\"work/Detachment/cls/model/epoch_4\"\r\n",
    "cls_model_dir=\"\"\r\n",
    "predict_img(test_dir,out_dir,result_dir,cls_model_dir=cls_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 打包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! zip -q -r result.zip Lesion_Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 集成提分\n",
    "\n",
    "看到这里是不是觉得很惊喜，很意外？上面的内容可以让得分有一点不一样了，但是压根不能让最后的得分达到目标值！因为分类器的存在，所有F1值都提升了！但是因为部分正例没有预测到，也就没有被分割，所以Dice得分又下降了！你很有可能得到的结果是这样的：\n",
    "\n",
    "|Score\t|Atrophy_F1\t|Atrophy_DICE\t|Detachment_F1\t|Detachment_DICE|\n",
    "| -------- | -------- | -------- |-------- | -------- |\n",
    "| 0.69962|\t0.94037|\t0.69938|\t0.61538\t|0.59552 |\n",
    "|0.7444\t|0.95369\t|0.81221\t|0.45455\t|0.73032|\n",
    "\n",
    "终究起来这是因为我们还需要对Det部分进行特别地优化处理！\n",
    "\n",
    "## Det数据的分析\n",
    "\n",
    "首先，我们需要对Det部分有一个良好地认识，比如先认识一下得分中的F1。如果我们不做分割，得分中的F1就会保持在0.03的水平，这时候recall=1，求解pre可以得到大约为0.015，一共400个样本，测试集共6个正例。有了这个信息，我们可以很轻松知道自己分类结果中究竟包含几个正例了。并且可以注意到，此时Dice得分很高，有0.85，所以只需要预测包含所有正例，做分割即可保证分割后Dice得分是满足目标的。\n",
    "\n",
    "然后，我们可以反推一下，自己的容错率是多少。即了解一下自己的分类结果中的FP最多是多少，大致可以知道3个FP成为了容错上限。\n",
    "\n",
    "现在，我们就可以~~快乐地~~构造分类器，进行集成了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 一些函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 特别章节，用于保存csv文件，方便后期投票学习，这里返回df方便了解究竟哪些样本被分为正例\r\n",
    "def get_det_cls_csv(test_dir,cls_model_dir,csv_dir):\r\n",
    "    os.makedirs('Detcsvfile', exist_ok=True)\r\n",
    "    if cls_model_dir==\"\":\r\n",
    "        cls_model_dir=out_dir+'/cls/model/best_model'\r\n",
    "    cls_model = pdx.load_model(cls_model_dir)\r\n",
    "    cls_list=[]\r\n",
    "    for item in os.listdir(test_dir):\r\n",
    "        fig_name=item.split('.')[0]\r\n",
    "        tmp=cls_model.predict(test_dir+'/'+item)[0]['category']\r\n",
    "        cls_list.append([fig_name,tmp])\r\n",
    "    df=pd.DataFrame(cls_list,columns=['img','type'])\r\n",
    "    df=df.sort_values(by='img')\r\n",
    "    df.to_csv('Detcsvfile/'+csv_dir+'.csv',index=None,header=None)\r\n",
    "    return df\r\n",
    "\r\n",
    "# 和前面的内容相比，这里更替了一下transform\r\n",
    "def generate_cls_model2(train_dir,eval_dir,label_dir,out_dir,root_dir='/home/aistudio',num_epochs=100):\r\n",
    "    train_transforms = pdx.transforms.Compose([\r\n",
    "        #T.Resize([500,600], interp='LINEAR', keep_ratio=False),\r\n",
    "        pdx.transforms.Resize([800,800]),\r\n",
    "        pdx.transforms.RandomHorizontalFlip(), \r\n",
    "        pdx.transforms.Normalize()])\r\n",
    "    eval_transforms = pdx.transforms.Compose([\r\n",
    "        #T.Resize([500,600], interp='LINEAR', keep_ratio=False),\r\n",
    "        pdx.transforms.Resize([800,800]),\r\n",
    "        pdx.transforms.Normalize()\r\n",
    "    ])\r\n",
    "\r\n",
    "    train_dataset = pdx.datasets.ImageNet(\r\n",
    "        data_dir=root_dir,\r\n",
    "        file_list=train_dir,\r\n",
    "        label_list=label_dir,\r\n",
    "        transforms=train_transforms,\r\n",
    "        shuffle=True)\r\n",
    "    eval_dataset = pdx.datasets.ImageNet(\r\n",
    "        data_dir=root_dir,\r\n",
    "        file_list=eval_dir,\r\n",
    "        label_list=label_dir,\r\n",
    "        transforms=eval_transforms)\r\n",
    "\r\n",
    "    num_classes = len(train_dataset.labels)\r\n",
    "    model = pdx.cls.ResNet101_vd_ssld(num_classes=num_classes)\r\n",
    "    model.train(num_epochs=num_epochs,\r\n",
    "                train_dataset=train_dataset,\r\n",
    "                train_batch_size=16,\r\n",
    "                eval_dataset=eval_dataset,\r\n",
    "                # lr_decay_epochs=[4, 6, 8],\r\n",
    "                save_interval_epochs=4, # int(num_epochs/5),\r\n",
    "                learning_rate=0.025,\r\n",
    "                save_dir=out_dir+'/cls/model',\r\n",
    "                use_vdl=False)\r\n",
    "\r\n",
    "# 这里构造一个一票通过制的分类结果集成\r\n",
    "# ResNet101_vd_ssld_10_6_3_6_0.83333_0.73032.csv是divide_type=3,epoch=6的训练结果，6个正例\r\n",
    "def predict_img3(test_dir,out_dir,result_dir,cls_model_dir=\"\"):\r\n",
    "    if cls_model_dir==\"\":\r\n",
    "        cls_model_dir=out_dir+'/cls/model/best_model'\r\n",
    "    \r\n",
    "    # 清空目录\r\n",
    "    myremovedirs(out_dir+'/pseudo_color_prediction')\r\n",
    "\r\n",
    "    os.makedirs(result_dir, exist_ok=True)\r\n",
    "    \r\n",
    "    test_list=[]\r\n",
    "    white_list=[]\r\n",
    "    \r\n",
    "    df=pd.read_csv('Detcsvfile/ResNet101_vd_ssld_10_6_3_6_0.83333_0.73032.csv',header=None)\r\n",
    "    df=df[df[1]==1]\r\n",
    "    check_list=np.array(df[0]).tolist()\r\n",
    "    \r\n",
    "    count=0\r\n",
    "    cls_model = pdx.load_model(cls_model_dir)\r\n",
    "    for item in os.listdir(test_dir):\r\n",
    "        fig_name=item.split('.')[0]\r\n",
    "        tmp=cls_model.predict(test_dir+'/'+item)[0]['category']\r\n",
    "        if tmp=='1': count=count+1\r\n",
    "        if tmp=='1' or fig_name in check_list:\r\n",
    "            test_list.append(test_dir+'/'+fig_name+'.jpg')\r\n",
    "        else:\r\n",
    "            white_list.append(test_dir+'/'+fig_name+'.jpg')\r\n",
    "    \r\n",
    "    print('empty img are '+str(len(white_list))+\", unempty img are \"+str(len(test_list))+', real unempyt img are '+str(count))\r\n",
    "\r\n",
    "\r\n",
    "    generate_white_pic(white_list,result_dir)\r\n",
    "\r\n",
    "    transforms = pds.transforms.Compose([\r\n",
    "        pds.transforms.Resize(target_size=(800, 800)),\r\n",
    "        pds.transforms.Normalize()\r\n",
    "        ])\r\n",
    "    \r\n",
    "    model = pds.models.UNet(num_classes=2)\r\n",
    "\r\n",
    "    predict(\r\n",
    "        model,\r\n",
    "        model_path = out_dir+'/det/model/best_model/model.pdparams',\r\n",
    "        transforms=transforms,\r\n",
    "        image_list=test_list,\r\n",
    "        save_dir=out_dir,)\r\n",
    "\r\n",
    "    for img in os.listdir(out_dir+'/pseudo_color_prediction'):\r\n",
    "        img_dir=out_dir+'/pseudo_color_prediction/'+img\r\n",
    "        im=Image.open(img_dir)\r\n",
    "        im=Image.fromarray(np.array(im)*255)\r\n",
    "        im.save(result_dir+'/'+img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 训练分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_dir='work/常规赛：PALM病理性近视病灶检测与分割/Train/fundus_image'\r\n",
    "lab_dir='work/常规赛：PALM病理性近视病灶检测与分割/Train/Lesion_Masks/Detachment'\r\n",
    "out_dir='work/Detachment'\r\n",
    "root_dir='/home/aistudio'\r\n",
    "\r\n",
    "train_dir,eval_dir,label_dir=generate_cls_file(img_dir,lab_dir,out_dir,divide_type=3)\r\n",
    "generate_cls_model2(train_dir,eval_dir,label_dir,out_dir,root_dir=root_dir,num_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_dir='work/Detachment'\r\n",
    "test_dir='work/常规赛：PALM病理性近视病灶检测与分割/PALM-Testing400-Images'\r\n",
    "result_dir='Lesion_Segmentation/Detachment'\r\n",
    "cls_model_dir=\"work/Detachment/cls/model/epoch_100\"\r\n",
    "# cls_model_dir=\"\"\r\n",
    "predict_img3(test_dir,out_dir,result_dir,cls_model_dir=cls_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 打包\n",
    "这次打包之后就可以获得目标的结果了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! zip -q -r result.zip Lesion_Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 一些结语\n",
    "\n",
    "我也曾想过要不要整理一下，按照Atr-Det的顺序写，但是决定还是按照 **通用分类分割 - 特别处理** 顺序写了。一方面是Unet分割模型在后期的集成部分没有重新训练，另一方面可能这种方式跟容易让大家理解我的思考顺序。\n",
    "\n",
    "回首自己跑出这个结果，觉得Paddle系列用的还是很开心的，轻松地用一些自己不是那么熟悉的内容，可以愉快地完成一些有意思的事情，但是使用过程中还是有很多迷茫，比如如何调参啥啥啥，模型如何使用啥啥啥，经常碰壁。甚至还想过说，把分类分割模型混在一起根据，同时训练，但是自己毕竟半路出家，也忙着毕业，就没整这些花里胡哨的操作。希望未来Paddle可以更好用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
